{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Régression & optimisation par descente de gradient\n",
    "\n",
    "Ce tp a deux objectifs: \n",
    " - Acquérir les connaissances de base pour faire face au problème de la régression, c'est à dire de l'estimation d'un score correpondant à une situation,\n",
    " - Comprendre la descente de gradient et ses variantes (stochastique, mini-batch, moment,...)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Chapitre 1: régression\n",
    "\n",
    "A partir de donnée étiquetées $\\{(x_i,y_i)\\}_{i=1,\\ldots,n}$, apprendre un modèle $f$ tel que $f(x)\\approx y$\n",
    "\n",
    "<img src=\"fig/reg_intro.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## A. Génération de données jouet & construction d'une solution analytique\n",
    "\n",
    "### A.1 Génération de données jouet \n",
    "\n",
    "[tout le code est donné... A vous de le comprendre]\n",
    "\n",
    "Dans un premier temps, générons des données jouets paramétriques:\n",
    "- $N$: nombre de points à générer\n",
    "  - $x \\in [0, 1]$ tirage avec un simple rand() ou un linspace() -au choix-\n",
    "  - Si vous optez pour un tirage aléatoire des abscisses, triez les points pour simplifier les traitements ultérieurs\n",
    "- $y=ax+b+\\epsilon, \\epsilon \\sim \\mathcal N(0,\\sigma^2)$\n",
    "  - Rappel : en multipliant un tirage aléatoire selon une gaussienne centrée réduite par $\\sigma$ on obtient le bruit décrit ci-dessus\n",
    "\n",
    "Afin de travailler sur les bonnes pratiques, nous distinguerons un ensemble d'apprentissage et un ensemble de test.\n",
    "Les deux sont tirés selon la même distribution. L'ensemble de test comptera -arbitrairement- 1000 points."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gen_data_lin(a, b, sig, N=500, Ntest=1000):\n",
    "    X_train = np.sort(np.random.rand(N)) # sort optionnel, mais ça aide pour les plots\n",
    "    X_test  = np.sort(np.random.rand(Ntest)) \n",
    "    Y_train = a*X_train + b + np.random.randn(N)*sig # a minima, bien comprendre les dimensions des vecteurs\n",
    "    Y_test  = a*X_test + b + np.random.randn(Ntest)*sig\n",
    "    return X_train, Y_train, X_test, Y_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# génération de données jouets:\n",
    "a = 6.\n",
    "b = -1.\n",
    "N = 50\n",
    "sig = .4 # écart type\n",
    "\n",
    "X_train, y_train, X_test, y_test = gen_data_lin(a, b, sig, N)\n",
    "\n",
    "plt.figure()\n",
    "#plt.plot(X_test, y_test, 'r.')\n",
    "plt.scatter(X_train, y_train, c='b')\n",
    "plt.grid()\n",
    "\n",
    "\n",
    "###  TODO  ###"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### A.2. Formulation au sens des moindres carrés\n",
    "\n",
    "Nous partons directement sur une écriture matricielle. Du coup, il est nécessaire de construire la matrice enrichie $X$:\n",
    "    $$X = \\begin{pmatrix}\n",
    "                x_0 & 1\\\\\n",
    "                \\vdots & \\vdots\\\\\n",
    "                x_n & 1\n",
    "                \\end{pmatrix}\n",
    "                 $$\n",
    "Le code de la fonction d'enrichissement est donné ci-dessous.\n",
    "\n",
    "\n",
    "L'idée de cet enrichissement est de simplifier le code suivant:\n",
    "1. facilité de construction de la fonction linéaire:\n",
    "$$ \\hat Y = X \\cdot w, \\qquad X\\in \\mathbb R^{n\\times 2}, \\qquad w = \\begin{pmatrix}a\\\\ b\\end{pmatrix}\n",
    "\\in \\mathbb R^{2}, \\qquad \\forall i,\\  \\hat y_i = a x_i + b$$\n",
    "2. L'idée est de minimiser l'erreur au sens des moindres carrés $C$:\n",
    "$$w^\\star = \\arg\\min_w C(X,Y,w) = \\arg\\min_w \\|X \\cdot w - Y\\|^2 $$\n",
    "3. Trouver l'argmin = annuler la dérivée du gradient:\n",
    "$$\\arg\\min_w \\|X \\cdot w - Y\\|^2 \\Leftrightarrow  \\nabla_w C(X,Y,w) = 2 X^T (X \\cdot w - Y) =0 \\Leftrightarrow  X^T X \\cdot w =  X^T Y $$ \n",
    "4. Le problème d'apprentissage revient alors à résoudre un système d'équations linéaires de la forme:\n",
    "$$ A w = B $$\n",
    "**Rappel des formules vues en cours/TD:**\n",
    "$$ A=X^T X \\in \\mathbb R^{d\\times d}$$\n",
    "$$ B=X^T Y\\in \\mathbb R^{d} $$\n",
    "Fonction de résolution: `np.linalg.solve(A,B)`\n",
    "Vous devez obtenir la même solution que précédemment. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_mat_lin_biais(X): # fonctionne pour un vecteur unidimensionel X\n",
    "    N = len(X)\n",
    "    return np.hstack((X.reshape(N,1),np.ones((N,1))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Xe = make_mat_lin_biais(X_train)\n",
    "# A compléter pour résoudre le problème d'apprentissage\n",
    "\n",
    "print(w)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### A.3. Evaluation des modèles\n",
    "\n",
    "Il est nécessaire d'avoir une évaluation **quantitative** (critère d'erreur MSE -Mean Squared Error-, MAPE -Mean Average Percentage Error-, ...). Sur les exemples simples, il est possible de tracer le résultat pour une analyse **qualitative**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluation quantitative\n",
    "\n",
    "def erreur_mc(y, yhat):\n",
    "    return ((y-yhat)**2).mean()\n",
    "\n",
    "# estimation des sorties avec les paramètres que vous avez calculé\n",
    "yhat_train = # a compléter\n",
    "yhat_test  = # a compléter\n",
    "\n",
    "print('Erreur moyenne au sens des moindres carrés (train):', erreur_mc(yhat_train, y_train))\n",
    "print('Erreur moyenne au sens des moindres carrés (test):', erreur_mc(yhat_test, y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluation qualitative\n",
    "\n",
    "plt.figure()\n",
    "plt.scatter(X_test, y_test, c='r', alpha=0.3)\n",
    "plt.scatter(X_test, yhat_test, c='g')\n",
    "plt.plot(X_test, yhat_test, 'g--')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## B. Construction de modèles non linéaire\n",
    "\n",
    "Dans le formalisme mis en place ci-dessus, il est très facile de passer à des modèles non linéaire, par exemple polynomiaux.\n",
    "\n",
    "1. Générer des données polynomiales pour valider votre approche\n",
    "2. A partir des données brutes $\\begin{pmatrix}x_0& x_1& \\ldots& x_n\\end{pmatrix}$, construction d'une matrice enrichie\n",
    " $$X = \\begin{pmatrix}\n",
    "                x_0^2 & x_0 & 1\\\\\n",
    "                \\vdots & \\vdots\\\\\n",
    "                x_n^2  & x_n & 1\n",
    "                \\end{pmatrix}\n",
    "                 $$\n",
    "\n",
    "3. Tout est fait ! \n",
    "$$ \\hat Y = X \\cdot w, \\qquad X\\in \\mathbb R^{n\\times 3}, \\qquad w = \\begin{pmatrix}a\\\\ b\\\\ c\\end{pmatrix}\n",
    "\\in \\mathbb R^{2}, \\qquad \\forall i,\\  \\hat y_i = a x_i^2 + b x_i +c$$\n",
    "\n",
    "En reprennant la formulation matricielle ci-dessus, on voit que naturellement $w=\\begin{pmatrix}a\\\\ b\\\\ c\\end{pmatrix}$ permet de définir les coefficients du polynome.\n",
    "\n",
    "OBJECTIF: <BR>\n",
    "**TRANSFORMER les données $\\Rightarrow$ TRANSFORMATION automatique du modèle**\n",
    "\n",
    "<p style=\"color:red\"> ATTENTION </p> \n",
    "\n",
    "Dans la suite, ne pas confondre l'ancien jeu de données `X_train, y_train` et le nouveau `Xp_train, yp_train`. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tout le code est donné\n",
    "def gen_data_poly2(a, b, c, sig, N=500, Ntest=1000):\n",
    "    '''\n",
    "    Tire N points X aléatoirement entre 0 et 1 et génère y = ax^2 + bx + c + eps\n",
    "    eps ~ N(0, sig^2)\n",
    "    '''\n",
    "    X_train = np.sort(np.random.rand(N))\n",
    "    X_test  = np.sort(np.random.rand(Ntest))\n",
    "    y_train = a*X_train**2+b*X_train+c+np.random.randn(N)*sig\n",
    "    y_test  = a*X_test**2 +b*X_test +c+np.random.randn(Ntest)*sig\n",
    "    return X_train, y_train, X_test, y_test\n",
    "\n",
    "Xp_train, yp_train, Xp_test, yp_test = gen_data_poly2(10, -10, 5, 0.1, N=100, Ntest=100)\n",
    "plt.figure()\n",
    "plt.plot(Xp_train, yp_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Enrichissement de la matrice X => pour construire un modèle polynomial\n",
    "def make_mat_poly_biais(X): \n",
    "    # A compléter\n",
    "    return Xe\n",
    "\n",
    "Xe   = make_mat_poly_biais(Xp_train)\n",
    "Xe_t = make_mat_poly_biais(Xp_test)\n",
    "w    = # A compléter \n",
    "\n",
    "yhat   = # A compléter\n",
    "yhat_t = # A compléter\n",
    "print('Erreur moyenne au sens des moindres carrés (train):', erreur_mc(yhat, yp_train))\n",
    "print('Erreur moyenne au sens des moindres carrés (test):', erreur_mc(yhat_t, yp_test))\n",
    "\n",
    "plt.figure()\n",
    "plt.plot(Xp_train, yp_train, 'b.')\n",
    "plt.plot(Xp_train, yhat, 'r')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Chapitre 2: Descente de gradient\n",
    "\n",
    "Se déplacer itérativement dans l'espace des paramètres (idéalement vers une solution optimale)\n",
    "\n",
    "<img src=\"fig/batch.png\">\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## C. Fonction de coût & optimisation par descente de gradient\n",
    "\n",
    "### C.1. Algorithme itératif\n",
    "\n",
    "Nous allons maintenant résoudre le problème de la régression par minimisation d'une fonction de coût:\n",
    "$$ C = \\sum_{i=1}^N (y_i - f(x_i))^2$$\n",
    "\n",
    "Soit un problème avec des données $(x_i,y_i)_{i=1,\\ldots,N}$, une fonction de décision/prédiction paramétrée par un vecteur $w$ et une fonction de cout à optimiser $C(w)$.\n",
    "Notre but est de trouver les paramètres $w^\\star$ minimisant la fonction de coût:\n",
    "$$ w^\\star = \\arg\\min_w C(w)$$\n",
    "\n",
    "l'algorithme de la descente de gradient est le suivant (rappel):\n",
    "\n",
    " - $w_0 \\leftarrow init$ par exemple : 0\n",
    " - boucle\n",
    "     - $w_{t+1} \\leftarrow w_{t} - \\epsilon \\nabla_w C(w_t)$\n",
    "\n",
    "Compléter le squelette d'implémentation fourni ci-dessous:\n",
    "\n",
    "----\n",
    "<p style=\"color:red\"> <strong>ATTENTION</strong> </p> \n",
    "\n",
    "Afin de pouvoir visualiser l'évolution des paramètres, nous allons travailler dans un espace à 2 paramètres, c'est à dire dans le cas de la **régression linéaire**: assurez-vous de bien travailler sur les données du premier exercice: `X_train, y_train`\n",
    "\n",
    "----\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pour travailler en matrice: (re)construction de la matrice contenant les X et un biais\n",
    "Xe = make_mat_lin_biais(X_train) # dataset linéaire, transformation lineaire des données\n",
    "wstar = # A compléter pour se rappeler du w optimal (code issu de la section précédente)\n",
    "\n",
    "def descente_grad_mc(X, y, eps=1e-4, nIterations=100):\n",
    "    w = np.zeros(X.shape[1]) # init à 0\n",
    "    allw = [w] # stocker toutes les valeurs que va prendre le gradient pour pouvoir l'afficher ensuite\n",
    "    for i in range(nIterations):\n",
    "        # A COMPLETER => calcul du gradient vu en TD\n",
    "        #\n",
    "        allw.append(w) # stockage de toutes les valeurs intermédiaires pour analyse\n",
    "    allw = np.array(allw)\n",
    "    return w, allw # la dernière valeur (meilleure) + tout l'historique pour le plot\n",
    "    \n",
    "w, allw = descente_grad_mc(Xe, y_train, eps=1e-4, nIterations=200)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "On s'intéresse ensuite à comprendre la descente de gradient dans l'espace des paramètres. Le code ci-dessous permet de tracer le cout pour un ensemble de paramètres (toutes les valeurs de paramètres prises par l'algorithmes au fil du temps).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tracer de l'espace des couts\n",
    "def plot_parametres( allw, X, y, opti = [], ngrid = 20, style ='b+-'):\n",
    "    '''\n",
    "    Fonction de tracer d'un historique de coefficients\n",
    "    ATTENTION: ca ne marche qu'en 2D (évidemment)\n",
    "    Chaque w doit contenir 2 valeurs\n",
    "    \n",
    "    Il faut fournir les données (X,y) pour calculer le cout associé \n",
    "    à un jeu de paramètres w\n",
    "    ATTENTION X = forme matricielle des données\n",
    "    '''\n",
    "    w_min = np.minimum(wstar, np.min(allw,0)) # trouver les bornes\n",
    "    w_max = np.maximum(wstar, np.max(allw,0))\n",
    "    w_min -= (w_max-w_min)*0.1\n",
    "    w_max += (w_max-w_min)*0.1\n",
    "    # faire une grille régulière avec tous les couples possibles entre le min et le max\n",
    "    w1range = np.linspace(w_min[0], w_max[0], ngrid)\n",
    "    w2range = np.linspace(w_min[1], w_max[1], ngrid)\n",
    "    w1,w2 = np.meshgrid(w1range,w2range)\n",
    "    #\n",
    "    # calcul de tous les couts associés à tous les couples de paramètres\n",
    "    cost = np.array([[np.log(((X @ np.array([w1i,w2j])-y)**2).sum()) for w1i in w1range] for w2j in w2range])\n",
    "    #\n",
    "    # plt.figure()\n",
    "    plt.contour(w1, w2, cost)\n",
    "    if len(opti) > 0:\n",
    "        plt.scatter(wstar[0], wstar[1],c='r')\n",
    "    plt.plot(allw[:,0],allw[:,1],style ,lw=2 )\n",
    "    return\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# invocation de la méthode définie ci-dessus\n",
    "plt.figure()\n",
    "plot_parametres( allw, Xe, y_train, opti=wstar)\n",
    "# plt.savefig('fig/grad_descente.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "###  TODO  ###"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Vous devez obtenir un image de la forme :\n",
    "![Descente de gradient](fig/grad_descente.png)\n",
    "\n",
    "### C.2 Comportement du gradient\n",
    "\n",
    "Tester différents jeux de paramètres pour mettre en évidence les phénomènes suivants:\n",
    " - Divergence du gradient\n",
    " - Convergence incomplète (trop lente ou pas assez d'itération)\n",
    " - Convergence idéale: pas de gradient suffisamment grand et nombre d'itérations bien choisi\n",
    "\n",
    " Vous devriez obtenir des situations comme:\n",
    " ![Descente de gradient](fig/cvg_grad.png)\n",
    " \n",
    "\n",
    "> <span style=\"color:magenta\"> Le réglage du pas de gradient (**=learning rate**) est une étape critique dans toutes les approches de type réseaux de neurones </span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Vos tests sur les paramètres de la descente de gradient"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### C.3 Détection & Implémentation de la convergence\n",
    "\n",
    "Jusqu'ici, on s'est contenté de descendre le grandient sur $N$ iterations... Il faut maintenant détecter la convergence pour sortir au bon moment:\n",
    "\n",
    "1. On garde une structure algorithmique en boucle `for`: il est important de garder un nombre d'itération max pour éviter les boucles infinies... On va juste ajouter un `if` + `break` pour sortir en cas de convergence.\n",
    "2. On sort quand?\n",
    "    - lorsque le gradient s'annule (ou se rapproche suffisamment de 0)\n",
    "    - lorsque le coût n'évolue plus (ou peu)\n",
    "    - **il y a donc une notion de seuil** $\\Rightarrow$ et donc un paramètre supplémentaire"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# on garde la méthode précédente qui marche... Et on en fait une nouvelle avec le critère de convergence\n",
    "def descente_grad_mc_cvg(X, y, eps=1e-4, cvg = 1e-3, nIterations=200):\n",
    "    w = np.zeros(X.shape[1]) # init à 0\n",
    "    allw = [w] # stocker toutes les valeurs que va prendre le gradient pour pouvoir l'afficher ensuite\n",
    "    for i in range(nIterations):\n",
    "        # Reprendre le code déjà fait\n",
    "        #\n",
    "        allw.append(w) # stockage de toutes les valeurs intermédiaires pour analyse\n",
    "        if # trouver un critère de convgergence\n",
    "            break\n",
    "\n",
    "    allw = np.array(allw)\n",
    "    return w, allw # la dernière valeur (meilleure) + tout l'historique pour le plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# test\n",
    "w, allw = descente_grad_mc_cvg(Xe, y_train, eps=1e-4,cvg = 1e-3, nIterations=200)\n",
    "# vérification graphique (c'est l'avantage de travailler en 2D)\n",
    "plt.figure()\n",
    "plot_parametres( allw, Xe, y_train, opti=wstar)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### C.4 Gradient stochastique (SGD)\n",
    "\n",
    "Rappel de l'idée générale: calculer une direction approximative du gradient sur un seul individu. La direction est moins chère à calculer mais il faut en faire plus.\n",
    "\n",
    "<span style=\"color:red\"> ATTENTION :</span> pour que ça marche, il faut voir les points dans un ordre aléatoire (d'où le nom stochastique)\n",
    "\n",
    "| Gradient batch | Gradient stochastique (SGD) |\n",
    "|--- | --- |\n",
    "| - $w_0 \\leftarrow init$ par exemple : 0 <BR> - boucle (itérations/convergence) <BR>  $\\qquad$ - $w_{t+1} \\leftarrow w_{t} - \\epsilon \\nabla_w C(X,Y,w_t)$  | - $w_0 \\leftarrow init$ par exemple : 0 <BR> - boucle (itérations/convergence) <BR>  $\\qquad$ <span style=\"color:red\"> - Tirage d'un index $i$ </span> <BR> $\\qquad$ - $w_{t+1} \\leftarrow w_{t} - \\epsilon \\nabla_w C(\\mathbf x_i,y_i,w_t)$  |\n",
    "\n",
    "1. Rappel des formules de gradient:\n",
    "\n",
    "$$C(X,Y,w) = \\|X \\cdot w - Y\\|^2 = \\sum_{i=1}^n C(\\mathbf x_i,y_i,w) = \\sum_{i=1}^n (\\mathbf x_i \\cdot w - y_i)^2$$\n",
    "$$\\nabla_w C(X,Y,w) = 2 X^T (X \\cdot w - Y)  \\qquad  \\nabla_w C(\\mathbf x_i,y_i,w) = 2 \\mathbf x_i^T (\\mathbf x_i \\cdot w - y_i) $$\n",
    "\n",
    "<span style=\"color:red\">Note: </span> Vous risquez d'avoir des problèmes avec les opérations matricielles en passant à la version stochastique. Il faut bien calculer les dimensions des différents éléments de la formule.\n",
    "\n",
    "2. Impact sur la convergence\n",
    "\n",
    "Vous allez obtenir un résultat de la forme:\n",
    "<img src=\"fig/grad_stoch.png\">\n",
    "\n",
    "Evidemment, la convergence est plus difficile à détecter. L'idée est de faire le test toutes les $n$ itérations (=**epoch**), c'est à dire une fois qu'on a vu tous les points (statistiquement).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# on garde la méthode précédente qui marche... Et on en fait une nouvelle avec le critère de convergence\n",
    "def descente_grad_mc_stoch(X, y, eps=1e-4, cvg = 1e-3, nIterations=200):\n",
    "    w = np.zeros(X.shape[1]) # init à 0\n",
    "    allw = [w] # stocker toutes les valeurs que va prendre le gradient pour pouvoir l'afficher ensuite\n",
    "    for i in range(nIterations):\n",
    "        # tirage d'un indice i\n",
    "        \n",
    "        # MAJ w\n",
    "\n",
    "        # stockage de w\n",
    "        allw.append(w) # stockage de toutes les valeurs intermédiaires pour analyse\n",
    "        # test de convergence (à faire en fin d'énoncé)\n",
    "\n",
    "    allw = np.array(allw)\n",
    "    return w, allw # la dernière valeur (meilleure) + tout l'historique pour le plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "###  TODO  ###"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### C.5 Gradient mini-batch\n",
    "\n",
    "Il s'agit logiquement d'une version intermédiaire: on calcule le gradient sur un sous-ensemble de points tirés aléatoirement (c'est ce qui est fait dans les réseaux de neurones modernes). On introduit un nouveau paramètre: **batch-size**.\n",
    "\n",
    "<span style=\"color:red\">Très proche des questions précédentes: vous pouvez garder cet exercice pour plus tard</span> "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# implémentation du gradient mini-batch\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### C.6 Moment supplémentaire, inertie dans le gradient stochastique\n",
    "\n",
    "Le blog de S. Ruder explique particulièrement bien les améliorations possibles sur les descentes de gradient.\n",
    "\n",
    "https://ruder.io/optimizing-gradient-descent/\n",
    "\n",
    "Prenons le premier exemple, pour ajouter de l'inertie dans la descente de gradient stochastique:\n",
    "\n",
    "$v_t = \\gamma v_{t-1} + (1-\\gamma) \\nabla_w C$ <BR>\n",
    "$w_t = w_{t-1} - \\epsilon v_t$\n",
    "\n",
    "1. Comment initialiser $v$?\n",
    "2. Quel impact sur la descente de gradient stochastique en fonction de $\\gamma$? Y a-t-il un cas limite permettant de revenir à la descente de gradient stochastique classique?\n",
    "\n",
    "<img src=\"fig/grad_stoch_mom.png\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Gradient stochastique à moment\n",
    "\n",
    "###  TODO  ###"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### C.7 [Bonus] Et si on affichait tout ça en 3D? Et en animation?\n",
    "\n",
    "De la 3D et de l'animation. Les fonctions ne marchent pas toutes parfaitement mais le résultat est sympa !\n",
    "\n",
    "<img src = \"fig/animation.gif\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from matplotlib import cm\n",
    "import time\n",
    "\n",
    "def plot_parametres_3d( allw, X, y, opti, figax,  ngrid = 20):\n",
    "    '''\n",
    "    Quick & Dirty...\n",
    "    '''\n",
    "    w_min = np.minimum(wstar, np.min(allw,0)) # trouver les bornes\n",
    "    w_max = np.maximum(wstar, np.max(allw,0))\n",
    "    w_min -= (w_max-w_min)*0.1 # ajouter 10%\n",
    "    w_max += (w_max-w_min)*0.1\n",
    "    # faire une grille régulière avec tous les couples possibles entre le min et le max\n",
    "    w1range = np.linspace(w_min[0], w_max[0], ngrid)\n",
    "    w2range = np.linspace(w_min[1], w_max[1], ngrid)\n",
    "    w1,w2 = np.meshgrid(w1range,w2range)\n",
    "    #\n",
    "    # calcul de tous les couts associés à tous les couples de paramètres\n",
    "    cost = np.array([[np.log(((X @ np.array([w1i,w2j])-y)**2).sum()) for w1i in w1range] for w2j in w2range])\n",
    "\n",
    "    costw = np.array([np.log(((X @ w-y)**2).sum()) for w in allw] )\n",
    "    costwstar = np.log(((X @ opti-y)**2).sum())\n",
    "    \n",
    "    #figax.plot_wireframe(w1, w2, cost,  cmap='viridis', rstride=1, cstride=1)\n",
    "    Z = (cost-cost.min())/(cost.max()-cost.min())\n",
    "    colors = cm.viridis(Z)\n",
    "    rcount, ccount, _ = colors.shape\n",
    "    surf = figax.plot_surface(w1, w2, cost,  rcount=rcount, ccount=ccount,\n",
    "                       facecolors=colors, shade=False)\n",
    "    surf.set_facecolor((0,0,0,0))\n",
    "    # if len(opti) > 0:\n",
    "    #     plt.scatter(wstar[0], wstar[1],c='r')\n",
    "    figax.plot(allw[:,0],allw[:,1],costw,c='m')\n",
    "    figax.scatter(opti[0],opti[1],costwstar,c='r', s=200)\n",
    "    return\n",
    "\n",
    "def plot_reg(X,y,w):\n",
    "    yhat = X@w\n",
    "    plt.scatter(X[:,0],y)\n",
    "    plt.plot(X[:,0],yhat,'m')\n",
    "    return\n",
    "\n",
    "Xe = make_mat_lin_biais(X_train) # dataset linéaire, transformation lineaire des données\n",
    "wstar = np.linalg.solve(Xe.T@Xe, Xe.T@y_train) # A compléter pour se rappeler du w optimal (code issu de la section précédente)\n",
    "\n",
    "fig = plt.figure(figsize=[16,8])\n",
    "#plt.subplot(1,2,1)\n",
    "ax = fig.add_subplot(1,2,1,projection='3d')\n",
    "w, allw = descente_grad_mc_stoch(Xe, y_train, eps=1e-1,  nIterations=200) # il faut disposer de la fonction !\n",
    "plot_parametres_3d( allw, Xe, y_train, wstar, ax)\n",
    "\n",
    "#plt.figure()\n",
    "fig.add_subplot(1,2,2)\n",
    "plot_reg(Xe,y_train,wstar)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creation d'une animation = sauvegarde dans un répertoire + gif animé sur le web\n",
    "w, allw = descente_grad_mc_stoch(Xe, y_train, eps=2e-1,  nIterations=200)\n",
    "fig = plt.figure(figsize=[16,8])\n",
    "\n",
    "for i,wi in enumerate(allw):\n",
    "    if i<50:\n",
    "            if i% 3 !=0:\n",
    "              continue        \n",
    "    elif i% 5 !=0:\n",
    "        continue\n",
    "    #plt.subplot(1,2,1)\n",
    "    ax = fig.add_subplot(1,2,1,projection='3d')\n",
    "    plot_parametres_3d( allw[:np.maximum(i,1)], Xe, y_train, wstar, ax)\n",
    "\n",
    "    #plt.figure()\n",
    "    fig.add_subplot(1,2,2)\n",
    "    plot_reg(Xe,y_train,wi)\n",
    "    fig.savefig(\"fig/anim/anim_\"+str(i)+\".png\")\n",
    "    fig.clear()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Chapitre 3: Passage sur des données réelles\n",
    "\n",
    "Après avoir étudié trois manières de faire face au problème de la régression, nous proposons d'étudier un cas réel: la prédiction de la consommation des voitures en fonction de leurs caractéristiques.\n",
    "\n",
    "Dans le cas présent, nous allons baser la solution sur la résolution analytique du problème des moindres carrés (`np.linalg.solve(A,B)`), qui semble la mieux adaptée au problème qui nous intéresse.\n",
    "\n",
    "Le jeu de données est issu des datasets UCI, un répertoire parmi les plus connus en machine learning. Les données **sont déjà téléchargées et présentes dans le tme** mais vous voulez plus d'informations:\n",
    "https://archive.ics.uci.edu/ml/datasets/auto+mpg\n",
    "\n",
    "![voiture](fig/Large9.jpg)\n",
    "\n",
    "Après avoir importé les données (fonction fournie), vous construirez une solution optimale et l'évaluerez au sens des moindres carrés en apprentissage et en test.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "# Chargement des données\n",
    "data = pd.read_csv('data/auto-mpg.data', delimiter='\\s+', header=None) # comme np.loadtxt mais en plus robuste\n",
    "# remplacement des données manquantes '?' => Nan pour travailler sur des nombres\n",
    "data.iloc[:,[3]] = data.iloc[:,[3]].replace('?', None)\n",
    "data.iloc[:,[3]] = data.iloc[:,[3]].astype(float)\n",
    "# remplacement des valeurs manquantes par la moyenne\n",
    "data.iloc[:,[3]] = data.iloc[:,[3]].fillna(data.iloc[:,[3]].mean())\n",
    "\n",
    "print(data.head()) # visualiser ce qu'il y a dans les données\n",
    "\n",
    "X = np.array(data.values[:,1:-2], dtype=np.float64)\n",
    "y = np.array(data.values[:,0], dtype=np.float64)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# separartion app/test\n",
    "def separation_train_test(X, y, pc_train=0.75):\n",
    "    # A compléter\n",
    "    # 1) générer tous les index entre 0 et N-1\n",
    "    # 2) mélanger la liste\n",
    "    napp = int(len(y)*pc_train) # nb de points pour le train\n",
    "    X_train, y_train = X[index[:napp]], y[index[:napp]]\n",
    "    X_test, y_test   = X[index[napp:]], y[index[napp:]]\n",
    "    return X_train, y_train, X_test, y_test\n",
    "\n",
    "X_train, y_train, X_test, y_test = separation_train_test(X, y, pc_train=0.75)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Résolution analytique\n",
    "\n",
    "w = # A compléter\n",
    "yhat   = # A compléter\n",
    "yhat_t = # A compléter\n",
    "print('Erreur moyenne au sens des moindres carrés (train):', erreur_mc(yhat, y_train))\n",
    "print('Erreur moyenne au sens des moindres carrés (test):', erreur_mc(yhat_t, y_test))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_y(y_train, y_test, yhat, yhat_t):\n",
    "    # tracé des prédictions:\n",
    "    plt.figure()\n",
    "    plt.subplot(211)\n",
    "    plt.plot(y_test, label=\"GT\")\n",
    "    plt.plot(yhat_t, label=\"pred\")\n",
    "    plt.title('En test')\n",
    "    plt.legend()\n",
    "    plt.subplot(212)\n",
    "    plt.plot(y_train, label=\"GT\")\n",
    "    plt.plot(yhat, label=\"pred\")\n",
    "    plt.title('En train')\n",
    "    return\n",
    "plot_y(y_train, y_test, yhat, yhat_t)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# interprétation des poids\n",
    "plt.figure()\n",
    "plt.bar(np.arange(len(w)), w)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Questions d'ouverture\n",
    "\n",
    "## Sélection de caractéristiques\n",
    "\n",
    "Quels sont les résultats obtenus en éliminant toutes les variables servent moins?\n",
    "\n",
    "## Feature engineering\n",
    "\n",
    "En étudiant la signification des variables du problèmes, on trouve:\n",
    "\n",
    "1. mpg: continuous \n",
    "2. cylinders: multi-valued discrete \n",
    "3. displacement: continuous \n",
    "4. horsepower: continuous \n",
    "5. weight: continuous \n",
    "6. acceleration: continuous \n",
    "7. model year: multi-valued discrete \n",
    "8. origin: multi-valued discrete \n",
    "\n",
    "D'après la question précédente, le poids, l'année du modèle et le biais sont des facteurs important pour le calcul de la consommation... Jusqu'ici, nous n'avons pas pris en compte l'origine qui était difficile à coder.\n",
    "\n",
    "### Encodage de l'origine\n",
    "\n",
    "La variable origine est accessible de la manière suivante:\n",
    "\n",
    "```\n",
    "  origine = data.values[:,-2]\n",
    "```\n",
    "Il faut le faire au début du traitement pour bien conserver la séparation en l'apprentissage et le test.\n",
    "\n",
    "Au moins les deux derniers facteurs discrets pourraient être traités différemment en one-hot encoding:\n",
    "$$X_j = x \\in \\{1, \\ldots, K\\} \\Rightarrow [0, 0, 1, 0] \\in \\{0, 1\\}^K$$\n",
    "\n",
    "La valeur $x$ donne l'index de la colonne non nulle.\n",
    "\n",
    "### Encodage de l'année\n",
    "\n",
    "Pour l'année, il est possible de procéder de la même manière, mais il préférable de découper les années en 10 catégories puis d'encoder pour limiter le nombre de dimensions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.7 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "vscode": {
   "interpreter": {
    "hash": "902a52bcf4503a473db011f1937bdfe17613b08622219712e0110e48c4958c23"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
