{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Canevas pour le TP Vélib\n",
    "\n",
    "**Travailler sur des données réelles**\n",
    "\n",
    "On n'a pas le temps de travailler sur des API réelles, on va sauter l'étape de récupération des données sur internet et aller directement à la phase d'analyse... Mais ces données n'en sont pas moins réelles et bruitées.\n",
    "\n",
    "Ce notebook permet aussi de consolider les acquis sur `pandas`, `numpy` ainsi que sur les fonctions d'affichage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# imports nécessaires dans le cadre du TP\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "try:\n",
    "    import seaborn as sns\n",
    "except:\n",
    "    !pip install seaborn\n",
    "    import seaborn as sns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import des données dans pandas\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# nom de fichier (chemin d'accès à personnaliser)\n",
    "fnamejs = 'ressources/dataVelibJSON.txt'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# lecture du fichier\n",
    "f= open(fnamejs,'r') # ouverture en lecture\n",
    "data = f.read() # récupération d'une chaine de caractères... Données vraiment très brutes !\n",
    "print(data[:500]) # voir ces données brutes (les 500 premiers caractères)\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Passage automatique à une table contenant une entrée par ligne\n",
    "\n",
    "**input** = '[{\"status\": \"OPEN\", \"contract_name\": \"Paris\", \"name\": \"\", \"bonus\": \"True\", \"bike_stands\": 50, \"number\": 31705, \"last_update\": 1410616150000, \"available_bike_stands\": 48, \"banking\": \"True\", \"available_bikes\": 1, \"address\": \"\", \"lat\": 48.8645278209514, \"lng\": 2.416170724425901, \"alt\": 74.37134552001953}, {\"status\": \"OPEN\", ...\n",
    "\n",
    "**output**=\n",
    " $$D =  \\left[\\begin{array}{cccc}\n",
    " ind & A & B & C \\\\1 & x_{1}& y_{1} &z_{1}\\\\ & \\vdots & \\\\\\ell&  x_{\\ell}& y_{\\ell} &z_{\\ell}\n",
    " \\\\ & \\vdots & \\\\ N&  x_{N}& y_{N} &z_{N}\n",
    " \\end{array}\n",
    " \\right]$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_json(data, orient='records')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Tester et comprendre le fonctionnement des méthodes suivantes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(df.head()) # entete + 5 lignes => comprendre les différentes variables aléatoires descriptives\n",
    "#print(df.index)\n",
    "df.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df.sort_index(axis=1, ascending=False)\n",
    "# methode de selection de donnée:\n",
    "df['lng']                   # recupération d'une colonne\n",
    "df.loc[:,['lat','lng']]     # recupération de deux colonnes\n",
    "\n",
    "df[df['lat']>48.85]         # sélection des données\n",
    "df['status'].value_counts() # récupération des valeurs possibles + comptage des occurences des valeurs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# récupération des données au format numpy\n",
    "df['alt'].to_numpy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <span style=\"color:red\"> EXO Transformation des données</span>\n",
    "\n",
    "Définir une nouvelle colonne correspondant aux arrondissements parisiens\n",
    "1. Nouvelle colonne:\n",
    "```df['arr'] = ...```\n",
    "1. Analyse de la colonne `number`: les deux premiers chiffres donnent l'arrondissement\n",
    "1. Donner le comptage des valeurs (nombre de stations par arrondissement)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "L'étude de cette variable fait apparaitre des stations hors des arrondissements... Nous allons les supprimer\n",
    "\n",
    "Cette opération est non-triviale: il faut sélectionner les stations $\\le$ 20.... Mais aussi les stations avec un arrondissement $>0$!\n",
    "\n",
    "Pour effectuer cette opération, le plus simple est d'utiliser `np.logical_and` qui agrège efficacement les booléens contenus dans deux vecteurs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pour travailler sur Paris uniquement:\n",
    "dfp =    # faire la sélection \n",
    "df = dfp # pour travailler sur df pendant tout le TP... le df d'origine est ici perdu."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# fonctions d'affichage\n",
    "\n",
    "Beaucoup de fonctions numpy/matplotlib/pandas sont compatibles entre elles. Démonstration ci-dessous:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.close('all') # tout fermer\n",
    "plt.figure() # nouvelle figure\n",
    "plt.scatter(df['lat'],df['lng']) # scatter plot\n",
    "plt.show() # affichage de la figure en cours\n",
    "\n",
    "# note: on peut obtenir le même genre de résultats avec plot, mais c'est moins joli !"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Etude de l'altitude\n",
    "\n",
    "1. Le code donne une couleur pour l'altitude des stations\n",
    "1. <span style=\"color:red\"> EXO</span> Modifier ce code (ou ouvrir une nouvelle boite) pour mettre en évidence l'arrondissement d'appartenance de la station\n",
    "1. <span style=\"color:red\"> EXO</span> Modifier ce code (ou ouvrir une nouvelle boite) pour mettre en évidence le taux de remplissage des stations\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots()\n",
    "coll = ax.scatter(df['lat'], df['lng'] , s=10, c=df['alt'], picker = 5)\n",
    "plt.grid(True) # affichage de la grille\n",
    "plt.colorbar(coll) # affichage de la légende\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Calcul d'aggrégat complexe\n",
    "\n",
    "La fonction crosstab de pandas permet de calculer des choses sur des facteurs croisés: on obtient ainsi des tables de contingence ou des statistiques plus avancées. \n",
    "Nous allons nous intéresser à la distribution des emplacements par arrondissement. Cet exemple est abordable car les distributions de probabilité sont dicrètes.\n",
    "\n",
    "https://pbpython.com/pandas-crosstab.html\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Combien de stations avec N places dans les différents arrondissement?')\n",
    "plt.figure()\n",
    "plt.imshow(pd.crosstab(df['arr'], df['bike_stands']))\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exercices\n",
    "\n",
    "Etudier la distribution de probabilité entre altitude et disponibilité. Cet exercice est un exercice à tiroirs... Comportant les étapes suivantes:\n",
    "\n",
    "1. Discrétisation de l'altitude pour travailler plus facilement\n",
    "1. Définition de la disponibilité en pourcentage (nb de dispo / nb de stands)\n",
    "1. Discrétisation de la disponibilité\n",
    "1. Calcul de la matrice de contingence (`crosstab`)\n",
    "1. Normalisation pour estimer la distribution $P(Alt, Dispo)$\n",
    "    * Quel problème se pose?\n",
    "1. Calcul de la distribution conditionnelle $P(Dispo | Alt)$\n",
    "1. Afficher les stations dans un repère altitude / disponibilité\n",
    "    * afficher la covariance de ces deux variables aléatoires et le coefficient de corrélation: proposer une interprétation des résultats\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Rappels sur le calcul d'une distribution conditionnelle\n",
    "\n",
    "Pour mieux distinguer les différents cas de figure de remplissage des stations, nous décidons de séparer les différentes classes d'altitude. Ainsi, nous allons caractériser la disponibilité des Vélibs dans différents univers.\n",
    "\n",
    "* Calculer P_D_A = \n",
    "$$P(D | A) = \\frac{P(D, A)}{P(A)}$$\n",
    "    * identifer les dimensions de la matrice cible\n",
    "    * vérifier que $\\forall i,\\ \\sum_j P(D = d_j | A= a_i) = 1$ [sanity check]\n",
    "* Calculer l'espérance de disponibilité sachant l'altitude E_D_A:\n",
    "$$\\forall i,\\qquad E[D|A = a_i] = \\sum_j d_j P(D = d_j | A= a_i)$$\n",
    "Comment s'interprète cette espérance?\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Paramétrisation de la discrétisation\n",
    "nA = 15\n",
    "nD = 12 # on ne prend pas les mêmes dimensions pour éviter de laisser trainer des bugs\n",
    "\n",
    "# discrétisation de l'altitude\n",
    "index = pd.cut(df['alt'], nA, labels=False)\n",
    "\n",
    "# visualiser la nouvelle variable pour comprendre le processus... Puis en faire une nouvelle colonne dans les données\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# calcul de la distribution jointe \n",
    "\n",
    "P_A_D =\n",
    "\n",
    "# calcul de la distribution conditionnelle\n",
    "\n",
    "P_D_sA ="
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure()\n",
    "plt.imshow(P_A_D) # affichage de la distribution jointe\n",
    "plt.colorbar()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# note: il existe des fonctions avancées dans la toolbox de stats qui font tous ces calculs automatiquement\n",
    "\n",
    "plt.figure()\n",
    "# ['ind_alt'] => indice d'altitude discrétisée ['ind_pc'] => indice de pourcentage de disponibilité\n",
    "sns.jointplot(df['ind_alt'],df['ind_pc'], kind=\"hex\", color=\"k\") \n",
    "# plt.savefig('sns_p_AD.pdf')\n",
    "\n",
    "# avec la regression\n",
    "plt.figure()\n",
    "# tracé en continu... On ne visualise pas vraiment la distribution jointe\n",
    "sns.jointplot(df['alt'],df['pc_available'], kind=\"reg\", color=\"k\")\n",
    "# plt.savefig('sns_p_AD_reg.pdf')\n",
    "\n",
    "\n",
    "# logique de filtrage sur les distributions continues\n",
    "plt.figure()\n",
    "# tracé en continu + lissage => on visualise bien la distribution jointe !\n",
    "sns.jointplot(df['alt'],df['pc_available'], kind=\"kde\", color=\"k\")\n",
    "# plt.savefig('sns_p_AD_smooth.pdf')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pour aller plus loin\n",
    "\n",
    "### Indépendance\n",
    "\n",
    "L'indépendance est un phénomène critique lors de l'implémentation des méthodes... Avec deux variables aléatoires, il suffit de tester:\n",
    "$$ X \\perp Y \\iff \\forall i,j p(X = x_{i}, Y=y_{j})  = p(X = x_{i})\\times p( Y=y_{j})$$\n",
    "... Ce qui n'est jamais vérifié exactement sur des données réelles.\n",
    "\n",
    "**La bonne question est donc: suis-je assez proche d'un phénomène d'indépendance?**\n",
    "\n",
    "#### taille des stations (S) VS arrondissements (Arr)\n",
    "\n",
    "* Etude de corrélation sur la taille des stations par rapport aux arrondissements\n",
    "    * tracé de la distribution jointe (sns.jointplot)\n",
    "    * calcul du coefficient de corrélation\n",
    "\n",
    "=> la faible valeur de coefficient de corrélation de corrélation nous donne un indice, mais nous nous rappelons que dans ce sens là, ce n'est pas une démonstration\n",
    "\n",
    "* Calcul d'indépendance exact:\n",
    "    * Discrétiser (ou plutot redistribuer) les tailles de stations sur 10 valeurs\n",
    "    * Calcul de la jointe P_ArrS (cf P_AD)\n",
    "        * Attention aux indices arr entre 1 et 20 => indices entre 0 et 19\n",
    "    * Calcul des marginales P_Arr, P_S (trivial à partir de la loi jointe)\n",
    "    * Calcul de PI_ArrS = P_Arr x P_S \n",
    "        * Implémentation du calcul par double boucle => trivial\n",
    "        * calcul matriciel => non trivial (il faut dessiner les matrices sur une feuille de brouillon)\n",
    "            * transformation des vecteurs en matrice + usage de dot <br>\n",
    "        PI_ArrS = P_Arr.reshape(Narr, 1).dot(P_S.reshape(1,Ns))\n",
    "        * Comparaison des valeurs PI_ArrS vs P_ArrS <br>\n",
    "        'diff = ((PI_ArrS - P_ArrS)**2).sum()'\n",
    "        => aucune chance d'arriver à 0...\n",
    "\n",
    "\n",
    "* Application du test de $\\chi^2$ = mesure d'une distance entre distribution\n",
    "    * Lien wikipedia : [https://fr.wikipedia.org/wiki/Test_du_χ²]\n",
    "    * Lien interne : [http://mapsi.lip6.fr/pmwiki.php?n=Cours.Semaine5]\n",
    "    * Mesure de la distance entre deux distributions  $P_t$ (distribution théorique, issue des marginales dans notre exemple) et $P_o$ (distribution jointe) \n",
    "    $$D = \\sum_{i}\\sum_j N \\frac{(P_t(i,j) - P_o(i,j))^2}{P_t(i,j)}$$\n",
    "    La mesure dépend du nombre d'observation $N$\n",
    "    * Chaque distribution est caractérisée par un nombre de degrés de libertés qui vaut ici: \n",
    "    $$DoF = (|Arr| - 1)(|S|-1) = 171$$\n",
    "    * La distance limite, avec $\\alpha$ de marge d'erreur, est donnée par :<br>\n",
    "    import scipy.stats as stats<br>\n",
    "    stats.chi2.ppf($\\alpha$, DoF)\n",
    "    * **Peut-on conclure que l'arrondissement est indépendant de la taille des stations?**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualisation de données, réduction de dimensionnalité\n",
    "\n",
    "Il serait intéressant de visualiser la population des stations en tenant compte de leur altitude, histoire de détecter les stations qui sont au bord des grandes montées.\n",
    "\n",
    "Il s'agit donc de visualiser des données 3D en 2D, donc de réduire la dimensionnalité des données.\n",
    "\n",
    "L'algorithme de l'état de l'art pour effectuer cette opération (délicate) est TSNE\n",
    "\n",
    "Les opérations à mener sont, dans l'ordre:\n",
    "1. Extraction des données au format numpy : pandas => numpy\n",
    "1. Normalisation des données (elles sont trop *serrées* par défaut et l'algorithme ne marche pas bien)\n",
    "    * **Seule chose restant à faire dans cet exercice**\n",
    "    * Colonne par colonne:\n",
    "        * Retirer la moyenne pour centrer la variable descriptive en 0\n",
    "        * Diviser par l'écart-type pour obtenir un écart-type unitaire sur la variable\n",
    "1. Invocation de TSNE pour passer de 3D à 2D\n",
    "1. Récupération des indices d'arrondissements pour faire un affichage plus joli (et surtout plus intelligible)\n",
    "1. Affichage\n",
    "\n",
    "Comment interpréter ce que vous avez sous les yeux?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import sklearn.manifold as visu # accès à TSNE\n",
    "\n",
    "# pour afficher les différents arrondissements de différentes couleurs (et forme)\n",
    "style = [(s,c) for s in \"o^<*\" for c in \"byrmck\" ]\n",
    "\n",
    "# passage dans un univers numpy\n",
    "X = df.loc[:,['lat','lng','alt']].as_matrix()\n",
    "A = df['arr'].as_matrix()\n",
    "# TODO : centrage des données en [0, 0, 0]\n",
    "# TODO : ecart type unitaire sur chacune des trois variables\n",
    "\n",
    "# réduction de la dimensionnalité\n",
    "X2d = visu.TSNE().fit_transform(X)\n",
    "plt.figure()\n",
    "indexes = np.unique(A)\n",
    "for i in range(len(indexes)): # affichage arrondissement par arrondissement\n",
    "    ind = indexes[i]\n",
    "    plt.scatter(X2d[A==ind,0],X2d[A==ind,1] , s=10,\\\n",
    "     marker=style[int(ind)%len(style)][0],c=style[int(ind)%len(style)][1])\n",
    "plt.legend(indexes)\n",
    "plt.savefig('stations_tsne_arr.pdf')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.7 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "vscode": {
   "interpreter": {
    "hash": "902a52bcf4503a473db011f1937bdfe17613b08622219712e0110e48c4958c23"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
